model:
  # Base model settings
  name: "gpt2"
  from_pretrained: true
  
  # Custom model settings (used if from_pretrained is false)
  custom_model:
    vocab_size: 50257  # GPT-2 vocab size
    n_positions: 128
    n_ctx: 128
    n_embd: 256
    n_layer: 6
    n_head: 8

training:
  # Training hyperparameters
  batch_size: 4
  num_epochs: 10
  learning_rate: 5e-5
  warmup_steps: 100
  max_length: 128
  context_max_length: 64
  save_every: 1
  
  # Optimizer settings
  weight_decay: 0.01
  adam_epsilon: 1e-8
  
  # Data settings
  val_split: 0.1
  seed: 42

generation:
  # Text generation parameters
  max_length: 100
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  num_return_sequences: 1
  do_sample: true

data:
  # Dataset paths relative to project root
  raw_data: "data/raw/physics_equations.json"
  processed_data: "data/processed/tokenized_equations.json"
  training_examples: "data/processed/training_examples.json"

evaluation:
  metrics:
    - accuracy
    - perplexity
  save_best_model: true
  model_save_path: "./models/best_model.pth"